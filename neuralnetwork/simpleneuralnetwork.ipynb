{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5521394b-74a8-48e8-9f2e-7a64da5fd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 11:30:31.427027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/peshal/opencv/build/lib\n",
      "2021-11-11 11:30:31.427065: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics \n",
    "import seaborn as sns \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f0ed02-8924-48b4-8af7-03ddbda891e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train),(X_test,Y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5782e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X,Y):\n",
    "    X=X.astype(np.float32)\n",
    "    X=X/255-0.5\n",
    "    X=X.reshape((len(X),-1))\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "    Y=lb.transform(Y) \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5625e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train=data_preprocessing(X_train,Y_train)\n",
    "X_test,Y_test=data_preprocessing(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630de819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_):\n",
    "        output=input_\n",
    "        return output_\n",
    "    def backward(self,input_,grad_output):\n",
    "        output_=np.dot(np.transpose(grad_output),input_)/len(grad_output)\n",
    "        return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59880c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_):\n",
    "        output_=np.where(input_>=0,input_,0)\n",
    "        return output_\n",
    "    def backward(self,input_,grad_output):\n",
    "        output_=np.where(input_>=0,1,0)\n",
    "        return output_*grad_output\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_):\n",
    "        output_=(1/(1+np.exp(-input_)))\n",
    "        return output_\n",
    "    def backward(self,input_,grad_output):\n",
    "        output_=input_*(1-input_)\n",
    "        return output_*grad_output\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_):\n",
    "        output_=(np.exp(input_)-np.exp(-input_))/(np.exp(input_)+np.exp(-input_))\n",
    "        return output_\n",
    "    def backward(self,input_,grad_output):\n",
    "        output_=(1-input_**2)\n",
    "        return output_*grad_output\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_):\n",
    "        exponential_of_input=np.exp(input_)\n",
    "        sum_along_row=np.sum(exponential_of_input,axis=1)\n",
    "        return np.divide(exponential_of_input,sum_along_row[:,None])\n",
    "    def backward(self,input_,grad_output):\n",
    "        return grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb26c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,input_nodes,output_nodes,learning_rate):\n",
    "        self.input_nodes=input_nodes+1\n",
    "        self.output_nodes=output_nodes\n",
    "        self.weights=np.random.normal(loc=0.0, scale = np.sqrt(2/(output_nodes+self.input_nodes)), size = (output_nodes,self.input_nodes))\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "    def forward(self,input_):\n",
    "        input_new=np.ones((len(input_),len(input_[0])+1))\n",
    "        input_new[:,1:]=np.copy(input_)\n",
    "        output_=np.dot(input_new,np.transpose(self.weights))\n",
    "        return output_\n",
    "\n",
    "    def backward(self,input_,grad_output):\n",
    "        input_new=np.ones((len(input_),len(input_[0])+1))\n",
    "        input_new[:,1:]=np.copy(input_)\n",
    "        grad_input=np.dot(grad_output,self.weights[:,1:])\n",
    "        gradient=np.dot(np.transpose(grad_output),input_new)/len(grad_output)\n",
    "        self.weights=self.weights-self.learning_rate*gradient\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0299097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(Yactual,Ypredict):\n",
    "    Y_actual=np.copy(Yactual)\n",
    "    Y_predict=np.copy(Ypredict)\n",
    "    loss_func=(Y_actual-Y_predict)**2\n",
    "    cost_func=np.sum(loss_func,axis=0)/len(loss_func)\n",
    "    return np.sum(cost_func)/len(cost_func)\n",
    "\n",
    "def MSE_gradient(Yactual,Ypredict):\n",
    "    return (Ypredict-Yactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4261ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(Yactual,Ypredict):\n",
    "    cross_entropy=0\n",
    "    Y_actual=np.argmax(Yactual,axis=1)\n",
    "    for i in range(len(Ypredict)):\n",
    "        j=Y_actual[i]\n",
    "        p=Ypredict[i][j]\n",
    "        if p<=0:\n",
    "            p=0.00000000000001\n",
    "        cross_entropy-=math.log(p)\n",
    "    return cross_entropy/len(Ypredict)\n",
    "    \n",
    "def categorical_cross_entropy_gradient(Yactual,Ypredict):\n",
    "    Y_actual=np.argmax(Yactual,axis=1)\n",
    "    Y_predict=np.copy(Ypredict)\n",
    "    for i in range(len(Y_predict)):\n",
    "        j=Y_actual[i]\n",
    "        Y_predict[i][j]=(Y_predict[i][j]-1)\n",
    "    grad_output=Y_predict\n",
    "    return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ad2476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Yactual,Ypredict):\n",
    "    Y_actual=np.copy(Yactual)\n",
    "    Y_predict=np.copy(Ypredict)\n",
    "    logistic_func=np.ones(Y_actual.shape)\n",
    "    for i in range(len(Y_predict)):\n",
    "        for j in range(0,len(Y_predict[0])):\n",
    "            value1=Y_predict[i][j]\n",
    "            value2=1-Y_predict[i][j]\n",
    "            if value1<=0 :\n",
    "                value1=0.00000000000001\n",
    "            elif value2<=0 :\n",
    "                value2=0.00000000000001\n",
    "            logistic_func[i][j]=(-Y_actual[i][j]*math.log(value1)-(1-Y_actual[i][j])*math.log(value2))\n",
    "    cost_func=np.sum(logistic_func,axis=0)/len(logistic_func)\n",
    "    return np.sum(cost_func)/len(cost_func)\n",
    "\n",
    "\n",
    "def cross_entropy_gradient(Yactual,Ypredict):\n",
    "    Y_actual=np.copy(Yactual)\n",
    "    Y_predict=np.copy(Ypredict)\n",
    "    Y_actual=np.where(Y_actual==0,0.00000000001,Y_actual)\n",
    "    Y_actual=np.where(Y_actual==1,0.99999999999,Y_actual)\n",
    "    Y_predict=np.where(Y_predict==0,0.00000000001,Y_predict)\n",
    "    Y_predict=np.where(Y_predict==1,0.99999999999,Y_predict)\n",
    "    actual_diff=-((Y_actual-Y_predict)/(Y_predict*(1-Y_predict)))\n",
    "    return actual_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbad2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of feed forward\n",
    "def forward(network,input_):\n",
    "    input_variable=np.copy(input_)\n",
    "    activation=[input_]\n",
    "    check_even_odd=0\n",
    "    for l in network:\n",
    "        input_variable=l.forward(input_variable)\n",
    "        check_even_odd+=1\n",
    "        if(check_even_odd%2==0):\n",
    "            activation.append(input_variable)\n",
    "    return activation\n",
    "# implementation of back propagation \n",
    "def backward(network,input_,grad_output):\n",
    "    activation_index=-1\n",
    "    for i in range(0,len(network)):\n",
    "        index=-(i+1)\n",
    "        if index%2==0:\n",
    "            activation_index-=1\n",
    "        grad_output=network[index].backward(input_[activation_index],grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d32b09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function \n",
    "def train(network,X_train,Y_train,epochs=100,batches=10):\n",
    "    number_of_trainingset=int(len(X_train)/batches)\n",
    "    for number_of_epochs in range(epochs):\n",
    "        mean_error=0\n",
    "        counts=[]\n",
    "        # based on batch sizes images are divided into batches or groups and are trained accrodingly\n",
    "        for batch in range(batches):\n",
    "            start=0\n",
    "            end=0;\n",
    "            if (batch+1)*number_of_trainingset < len(X_train):\n",
    "                start=int(batch*number_of_trainingset)\n",
    "                end=int(start+number_of_trainingset)\n",
    "            else:\n",
    "                start=int(batch*number_of_trainingset)\n",
    "                end=int(len(X_train))\n",
    "            X_train_batch=X_train[start:end,:]\n",
    "            Y_train_batch=Y_train[start:end,:]\n",
    "            activation_output=forward(network=network,input_=X_train_batch)\n",
    "            # Loss function\n",
    "            mean_error=cross_entropy(Y_train_batch,activation_output[-1])\n",
    "            # gradient of loss function\n",
    "            grad_output=cross_entropy_gradient(Y_train_batch,activation_output[-1])\n",
    "            # Backpropagation\n",
    "            backward(network=network,input_=activation_output,grad_output=grad_output)\n",
    "            Y_predict=np.argmax(activation_output[-1],axis=1)\n",
    "            Y_train_batch_one_dimesional=np.argmax(Y_train_batch,axis=1)\n",
    "            unique_value,counts=np.unique(Y_predict==Y_train_batch_one_dimesional,return_counts=True)\n",
    "        print(\"error: \"+mean_error.__str__()+\"  accuracy : \"+( counts[1]/np.sum(counts)*100).__str__()+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fa8563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing function \n",
    "def test(network,X_test,Y_test):\n",
    "    activation_output=forward(network=network,input_=X_test)\n",
    "    mean_error=cross_entropy(Y_test,activation_output[-1])\n",
    "    Y_predict=np.argmax(activation_output[-1],axis=1)\n",
    "    Y_test=np.argmax(Y_test,axis=1)\n",
    "    unique_value,counts=np.unique(Y_predict==Y_test,return_counts=True)\n",
    "    print(\"error: \"+mean_error.__str__()+\"  accuracy : \"+( counts[1]/np.sum(counts)*100).__str__()+\"%\")\n",
    "    cf_matrix=metrics.confusion_matrix(Y_test,Y_predict, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "    print(metrics.classification_report(Y_test,Y_predict, labels=[0,1,2,3,4,5,6,7,8,9],zero_division=0))\n",
    "    sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "    sns.heatmap(cf_matrix, annot=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ce52371",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=2\n",
    "#creating neural network model with sigmoid as activation function \n",
    "network=[]\n",
    "network.append(Dense(input_nodes=784,output_nodes=15,learning_rate=learning_rate))\n",
    "network.append(Sigmoid())\n",
    "network.append(Dense(input_nodes=15,output_nodes=11,learning_rate=learning_rate))\n",
    "network.append(Sigmoid())\n",
    "network.append(Dense(input_nodes=11,output_nodes=10,learning_rate=learning_rate))\n",
    "network.append(Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(network=network,X_train=X_train,Y_train=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(network=network,X_test=X_test,Y_test=Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
